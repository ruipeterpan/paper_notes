---
description: >-
  The field of MLSys can be broken into two parts: Systems for Machine Learning,
  and Machine Learning for Systems.
---

# Machine Learning Systems - Index

### Meta-papers

* [A Berkeley View of Systems Challenges for AI](https://thodrek.github.io/CS839_spring18/papers/EECS-2017-159.pdf)
* [MLSys: The New Frontier of Machine Learning Systems](https://arxiv.org/pdf/1904.03257.pdf)

### Reading Lists

* [CS 294 @ Berkeley: Machine Learning Systems](https://ucbrise.github.io/cs294-ai-sys-fa19/)
* [CS 744 @ UW-Madison: Big Data Systems](http://pages.cs.wisc.edu/~shivaram/cs744-fa20/)

### TODOs

| To move from local note to gitbook | To read |
| :--- | :--- |
| MapReduce: Simplified Data Processing on Large Clusters | ByteScheduler: A Generic Communication Scheduler for DistributedDNN Training Acceleration |
| Parameter Server: Scaling Distributed Machine Learning with the Parameter Server | Gandiva: Introspective Cluster Scheduling for Deep Learning |
| PyTorch Distributed: Experiences on Accelerating Data Parallel Training | Horovod: Fast and Easy Distributed Deep Learning in TensorFlow |
| Themis: Fair and Efficient GPU Cluster Scheduling | Tiresias: A GPU Cluster Manager for Distributed Deep Learning |
| Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads | TensorFlow: A system for large-scale machine learning |
|  | In-Datacenter Performance Analysis of a Tensor Processing Unit |
|  | [PipeDream: Generalized Pipeline Parallelism for DNN Training](https://cs.stanford.edu/~matei/papers/2019/sosp_pipedream.pdf) |
|  |  |

### Table of Contents

#### Subarea 1

| Paper Title | Link |
| :--- | :--- |
|  |  |
|  |  |

#### Subarea 2

#### Subarea 3

#### Subarea 4

### Links

