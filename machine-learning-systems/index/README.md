# Machine Learning Systems - Index

## Meta stuff

* Reading lists
  * [CS 294 @ Berkeley: Machine Learning Systems](https://ucbrise.github.io/cs294-ai-sys-fa19/)
  * [CS 744 @ UW-Madison: Big Data Systems](http://pages.cs.wisc.edu/~shivaram/cs744-fa20/)
  * [CSE 559W @ U Washington Slides](http://dlsys.cs.washington.edu/schedule)
* Some other stuff
  * Meta papers
    * [A Berkeley View of Systems Challenges for AI](https://thodrek.github.io/CS839_spring18/papers/EECS-2017-159.pdf)
    * [MLSys: The New Frontier of Machine Learning Systems](https://arxiv.org/pdf/1904.03257.pdf)
  * [Systems Benchmarking Crimes](https://www.cse.unsw.edu.au/~gernot/benchmarking-crimes.html)

## Table of Contents

### Infrastructure

| Title | Venue |
| :--- | :--- |
| [NFS: Sun's Network File System](../../operating-systems/index/nfs-suns-network-file-system.md) | USENIX '86 |
| The Google File System | SOSP '03 |
| MapReduce: Simplified Data Processing on Large Clusters | OSDI '04 |
| The Hadoop Distributed File System | ??? '07 |
| Spark: Cluster Computing with Working Sets | HotCloud '10 |

### Scheduling

| Title | Venue |
| :--- | :--- |
| The Power of Choice in Data-aware Cluster Scheduling | OSDI '14 |
| [Gandiva: Introspective Cluster Scheduling for Deep Learning](gandiva-introspective-cluster-scheduling-for-deep-learning.md) | OSDI '18 |
| Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters | EuroSys '18 |
| [Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads](analysis-of-large-scale-multi-tenant-gpu-clusters-for-dnn-training-workloads.md) | ATC '19 |
| [Tiresias: A GPU Cluster Manager for Distributed Deep Learning](tiresias-a-gpu-cluster-manager-for-distributed-deep-learning.md) | NSDI '19 |
| [Themis: Fair and Efficient GPU Cluster Scheduling](themis-fair-and-efficient-gpu-cluster-scheduling.md) | NSDI '20 |

### Machine Learning

| Title | Venue |
| :--- | :--- |
| [Scaling Distributed Machine Learning with the Parameter Server](scaling-distributed-machine-learning-with-the-parameter-server.md) | OSDI '14 |
| TensorFlow: A system for large-scale machine learning | OSDI '16 |
| Clipper: A Low-Latency Online Prediction Serving System | NSDI '17 |
| Ray: A Distributed Framework for Emerging AI Applications | OSDI '18 |
| Horovod: Fast and Easy Distributed Deep Learning in TensorFlow | arXiv '18 |
| Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training | ICLR '18 |
| PyTorch: An Imperative Style, High-Performance Deep Learning Library | NeurIPS '19 |
| ByteScheduler: A Generic Communication Scheduler for Distributed DNN Training Acceleration | SOSP '19 |
| [BytePS: A High Performance and Generic Framework for Distributed DNN Training](byteps-a-high-performance-and-generic-framework-for-distributed-dnn-training.md) | OSDI '20 |
| [PyTorch Distributed: Experiences on Accelerating Data Parallel Training](pytorch-distributed-experiences-on-accelerating-data-parallel-training.md) | VLDB '20 |

### SQL Frameworks

| Title | Venue |
| :--- | :--- |
| Spark SQL: Relational Data Processing in Spark | SIGMOD '15 |

### Stream Processing

| Title | Venue |
| :--- | :--- |
| The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing | VLDB '15 |

### Graph Processing

| Title | Venue |
| :--- | :--- |
| PowerGraph: Distributed Graph-Parallel Computation on Neural Graphs | OSDI '12 |

### New Data, Hardware Models

| Title | Venue |
| :--- | :--- |
| In-Datacenter Performance Analysis of a Tensor Processing Unit | ISCA '17 |

## To Read

### To move from local note to GitBook

* [ ] MapReduce: Simplified Data Processing on Large Clusters
* [x] Parameter Server: Scaling Distributed Machine Learning with the Parameter Server
* [x] Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads

### To read

* [ ] ByteScheduler: A Generic Communication Scheduler for Distributed DNN Training Acceleration
* [x] Gandiva: Introspective Cluster Scheduling for Deep Learning
* [ ] Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters
* [ ] Horovod: Fast and Easy Distributed Deep Learning in TensorFlow
* [x] Tiresias: A GPU Cluster Manager for Distributed Deep Learning
* [ ] TensorFlow: A system for large-scale machine learning
* [ ] In-Datacenter Performance Analysis of a Tensor Processing Unit
* [ ] PipeDream: Generalized Pipeline Parallelism for DNN Training
* [ ] CNTK: Microsoftâ€™s Open-Source Deep-Learning Toolkit
* [ ] Caffe: Convolutional Architecture for Fast Feature Embedding
* [ ] PyTorch: An Imperative Style, High-Performance Deep Learning Library
* [ ] The Hadoop Distributed File System
* [ ] Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training
* [ ] The Power of Choice in Data-aware Cluster Scheduling
* [ ] [Spark: Cluster Computing with Working ](https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf)Sets

