# HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent

## One-line Summary

> If you understand this, you get the basic idea. But as with all complicated systems, the devil is in the details.

## Paper Structure Outline

1. 
## Background & Motivation

## Design and Implementation

## Evaluation

## New Vocabulary

* 
## Links

* [Paper PDF](https://papers.nips.cc/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf)
* Presentation video at xxx
* Presentation slides
* [Hogwild in PyTorch](https://pytorch.org/docs/stable/notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild)
* xxx on GitHub
* 
